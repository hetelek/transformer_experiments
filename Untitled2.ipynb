{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import io\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 10\n",
    "input_text = 'text.txt'\n",
    "\n",
    "char_to_token = {}\n",
    "token_to_char = {}\n",
    "def load_char_to_token():\n",
    "    global char_to_token\n",
    "    with io.open(input_text, 'r', encoding='utf-8') as f:\n",
    "        while True:\n",
    "            c = f.read(1)\n",
    "            if not c:\n",
    "                break\n",
    "            \n",
    "            if c not in char_to_token:\n",
    "                next_token = len(char_to_token)\n",
    "                char_to_token[c] = next_token\n",
    "                token_to_char[next_token] = c\n",
    "load_char_to_token()\n",
    "\n",
    "def tokens_from_file(path):\n",
    "    tokens = []\n",
    "    with io.open(input_text, 'r', encoding='utf-8') as f:\n",
    "        tokens = [[char_to_token[c] for c in f.read()]]\n",
    "    return torch.LongTensor(tokens)\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    if isinstance(tokens, torch.Tensor):\n",
    "        return ''.join([token_to_char[t.item()] for t in tokens.squeeze(0)])\n",
    "    else:\n",
    "        return ''.join([token_to_char[t] for t in tokens])\n",
    "\n",
    "_embedding = nn.Embedding(len(char_to_token), EMBEDDING_SIZE)\n",
    "_embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "def get_embedding_from_str(in_str):\n",
    "    tokens = [[char_to_token[c]] for c in in_str]\n",
    "    return _embedding(torch.LongTensor(tokens))\n",
    "\n",
    "def get_embedding(tensor):\n",
    "    return _embedding(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tests\n",
    "file_tokens = tokens_from_file(input_text)\n",
    "assert torch.all(torch.eq(file_tokens[0, :10], torch.Tensor([[0, 0, 0, 0, 1, 2, 3, 4, 5, 6]])))\n",
    "assert tokens_to_string([1, 2, 3, 4, 23]) == 'â„¢ðŸ˜€Thf'\n",
    "assert tokens_to_string(file_tokens).startswith('\\x00\\x00\\x00\\x00â„¢ðŸ˜€This â„¢is a tutorial on how to train a seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = 1\n",
    "src_mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "src_mask = src_mask.float().masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0.0))\n",
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nlayers = 1\n",
    "nhead = 10\n",
    "nhid = 1\n",
    "dropout = 0.0\n",
    "\n",
    "pe = PositionalEncoding(EMBEDDING_SIZE, dropout=dropout)\n",
    "encoder_layers = TransformerEncoderLayer(EMBEDDING_SIZE, nhead, nhid, dropout)\n",
    "transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "decoder = nn.Linear(EMBEDDING_SIZE, len(char_to_token))\n",
    "decoder.bias.data.zero_()\n",
    "decoder.weight.data.uniform_(-0.1, 0.1)\n",
    "softmax_layer = nn.Softmax(dim=2)\n",
    "\n",
    "def forward_pass(input_tokens):\n",
    "    input_x = get_embedding(input_tokens)\n",
    "    positional_embedding = pe(input_x)\n",
    "\n",
    "    assert input_x.size() == torch.Size([1, 15, 10])\n",
    "    assert positional_embedding.size() == torch.Size([1, 15, 10])\n",
    "    assert target_y.size()[0] == input_x.size()[1]\n",
    "    \n",
    "    output = transformer_encoder(positional_embedding, src_mask)  # src_mask ?\n",
    "\n",
    "    # why is this `torch.Size([1, 750, 52])` ? I want the output to be the next character `torch.Size([1, 1, 52])`\n",
    "    output = decoder(output)\n",
    "\n",
    "    assert output.size()[1] == target_y.size()[0]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def full_forward_pass(input_tokens):\n",
    "    output = forward_pass(input_tokens)\n",
    "    output = softmax_layer(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1.0 # learning rate\n",
    "all_params = list(_embedding.parameters()) + list(transformer_encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.SGD(all_params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0212e+00, -1.1097e+00, -1.0430e+00, -9.9356e-01,  2.2280e+00,\n",
      "          -3.5028e-01, -1.1525e+00,  8.6143e+00,  8.7353e+00,  8.8158e+00,\n",
      "           8.7948e+00,  3.0072e+00, -2.6350e+00,  2.1460e+00,  3.0655e+00,\n",
      "          -1.0365e+00, -1.1486e+00, -9.6878e-01, -1.0518e+00, -1.0266e+00,\n",
      "          -1.0444e+00, -1.1298e+00, -1.0688e+00, -1.0958e+00, -1.0066e+00,\n",
      "          -1.0627e+00, -1.1168e+00, -9.5932e-01, -9.7909e-01, -1.2230e+00,\n",
      "          -1.0078e+00, -9.1004e-01, -9.9655e-01, -1.1312e+00, -9.7119e-01,\n",
      "          -1.1217e+00, -1.0642e+00, -1.0334e+00, -1.0901e+00, -1.0834e+00,\n",
      "          -9.3505e-01, -1.1007e+00, -1.0330e+00, -9.5698e-01, -1.1136e+00,\n",
      "          -9.9115e-01, -1.1085e+00, -1.0790e+00, -1.0739e+00, -1.0641e+00,\n",
      "          -9.1462e-01, -1.0837e+00],\n",
      "         [-1.0212e+00, -1.1097e+00, -1.0430e+00, -9.9356e-01,  2.2280e+00,\n",
      "          -3.5028e-01, -1.1525e+00,  8.6143e+00,  8.7353e+00,  8.8158e+00,\n",
      "           8.7948e+00,  3.0072e+00, -2.6350e+00,  2.1460e+00,  3.0655e+00,\n",
      "          -1.0365e+00, -1.1486e+00, -9.6878e-01, -1.0518e+00, -1.0266e+00,\n",
      "          -1.0444e+00, -1.1298e+00, -1.0688e+00, -1.0958e+00, -1.0066e+00,\n",
      "          -1.0627e+00, -1.1168e+00, -9.5932e-01, -9.7909e-01, -1.2230e+00,\n",
      "          -1.0078e+00, -9.1004e-01, -9.9655e-01, -1.1312e+00, -9.7119e-01,\n",
      "          -1.1217e+00, -1.0642e+00, -1.0334e+00, -1.0901e+00, -1.0834e+00,\n",
      "          -9.3505e-01, -1.1007e+00, -1.0330e+00, -9.5698e-01, -1.1136e+00,\n",
      "          -9.9115e-01, -1.1085e+00, -1.0790e+00, -1.0739e+00, -1.0641e+00,\n",
      "          -9.1462e-01, -1.0837e+00],\n",
      "         [-1.0212e+00, -1.1097e+00, -1.0430e+00, -9.9356e-01,  2.2280e+00,\n",
      "          -3.5028e-01, -1.1525e+00,  8.6143e+00,  8.7353e+00,  8.8158e+00,\n",
      "           8.7948e+00,  3.0072e+00, -2.6350e+00,  2.1460e+00,  3.0655e+00,\n",
      "          -1.0365e+00, -1.1486e+00, -9.6878e-01, -1.0518e+00, -1.0266e+00,\n",
      "          -1.0444e+00, -1.1298e+00, -1.0688e+00, -1.0958e+00, -1.0066e+00,\n",
      "          -1.0627e+00, -1.1168e+00, -9.5932e-01, -9.7909e-01, -1.2230e+00,\n",
      "          -1.0078e+00, -9.1004e-01, -9.9655e-01, -1.1312e+00, -9.7119e-01,\n",
      "          -1.1217e+00, -1.0642e+00, -1.0334e+00, -1.0901e+00, -1.0834e+00,\n",
      "          -9.3505e-01, -1.1007e+00, -1.0330e+00, -9.5698e-01, -1.1136e+00,\n",
      "          -9.9115e-01, -1.1085e+00, -1.0790e+00, -1.0739e+00, -1.0641e+00,\n",
      "          -9.1462e-01, -1.0837e+00],\n",
      "         [-1.0212e+00, -1.1097e+00, -1.0430e+00, -9.9356e-01,  2.2280e+00,\n",
      "          -3.5028e-01, -1.1525e+00,  8.6143e+00,  8.7353e+00,  8.8158e+00,\n",
      "           8.7948e+00,  3.0072e+00, -2.6350e+00,  2.1460e+00,  3.0655e+00,\n",
      "          -1.0365e+00, -1.1486e+00, -9.6878e-01, -1.0518e+00, -1.0266e+00,\n",
      "          -1.0444e+00, -1.1298e+00, -1.0688e+00, -1.0958e+00, -1.0066e+00,\n",
      "          -1.0627e+00, -1.1168e+00, -9.5932e-01, -9.7909e-01, -1.2230e+00,\n",
      "          -1.0078e+00, -9.1004e-01, -9.9655e-01, -1.1312e+00, -9.7119e-01,\n",
      "          -1.1217e+00, -1.0642e+00, -1.0334e+00, -1.0901e+00, -1.0834e+00,\n",
      "          -9.3505e-01, -1.1007e+00, -1.0330e+00, -9.5698e-01, -1.1136e+00,\n",
      "          -9.9115e-01, -1.1085e+00, -1.0790e+00, -1.0739e+00, -1.0641e+00,\n",
      "          -9.1462e-01, -1.0837e+00],\n",
      "         [-9.6297e-01, -9.3602e-01, -7.6290e-01, -7.2349e-01, -2.0353e+00,\n",
      "           1.0515e+00, -4.9027e-01,  4.4867e+00,  3.8657e+00,  1.1224e+01,\n",
      "           3.5668e+00,  1.1763e+01,  1.6408e+00, -6.1633e-01, -9.9112e-01,\n",
      "          -5.5415e-01, -7.9784e-01, -9.0528e-01, -7.7497e-01, -8.0196e-01,\n",
      "          -9.8720e-01, -1.0700e+00, -7.8248e-01, -8.7676e-01, -7.3629e-01,\n",
      "          -7.6334e-01, -8.3274e-01, -8.2520e-01, -5.4213e-01, -1.2211e+00,\n",
      "          -9.7423e-01, -6.8834e-01, -7.8141e-01, -8.5002e-01, -7.9255e-01,\n",
      "          -1.0340e+00, -6.4829e-01, -8.4869e-01, -1.0019e+00, -8.4700e-01,\n",
      "          -8.0604e-01, -9.2880e-01, -9.4466e-01, -6.9991e-01, -5.9585e-01,\n",
      "          -5.9806e-01, -1.0127e+00, -1.0409e+00, -9.4517e-01, -8.7740e-01,\n",
      "          -7.8078e-01, -8.5869e-01],\n",
      "         [-4.7746e-01, -1.2134e-01, -1.6956e-01, -3.0448e-01, -1.3573e+00,\n",
      "           5.1577e+00,  2.6984e-01,  2.1147e+00, -3.8140e+00,  6.2771e+00,\n",
      "          -6.1446e+00,  1.4128e+01,  5.1366e+00, -2.4116e-01, -5.7568e+00,\n",
      "          -2.8125e-02, -1.5886e-01, -5.8624e-01, -3.3539e-01, -4.1358e-01,\n",
      "          -6.6642e-01, -5.0417e-01, -2.6643e-01, -1.8071e-02, -2.8089e-01,\n",
      "          -3.8083e-01,  1.1799e-01, -3.2533e-01, -2.3103e-01, -4.7098e-01,\n",
      "          -6.8209e-01, -4.1176e-01, -1.5503e-01, -2.3150e-01, -5.1281e-01,\n",
      "          -6.2839e-01, -1.1766e-01, -4.8338e-01, -3.7286e-01, -5.1804e-01,\n",
      "          -4.3879e-01, -3.4254e-01, -5.6716e-01, -4.0100e-01,  3.2528e-02,\n",
      "          -9.6996e-02, -4.0744e-01, -6.4948e-01, -5.1269e-01, -4.5207e-01,\n",
      "          -5.7190e-01, -5.3982e-01],\n",
      "         [-4.9971e-01, -4.8986e-01, -4.7097e-01, -3.8769e-01, -1.2600e-01,\n",
      "           1.4758e+00, -3.5615e-01,  2.3315e+00,  1.6975e+00,  1.6106e+00,\n",
      "          -5.1513e+00,  2.5925e+00,  1.2635e+01, -1.0731e+00,  1.7868e+00,\n",
      "          -2.5276e-01,  2.2988e-02, -2.5172e-01, -8.5998e-03, -8.8526e-02,\n",
      "          -3.5736e-01, -2.8385e-01, -1.1589e-01, -7.0021e-01, -3.7344e-01,\n",
      "          -1.7283e-01, -5.8350e-01, -6.5645e-01, -4.9676e-01, -3.6863e-01,\n",
      "          -3.1414e-01, -2.9939e-01, -5.8500e-01, -9.5644e-02, -5.3642e-01,\n",
      "          -1.0596e-01, -1.6910e-01, -1.0630e-01, -4.9445e-01, -4.9337e-02,\n",
      "          -5.1968e-01, -1.7253e-01, -4.1671e-01, -3.2217e-01, -2.5740e-01,\n",
      "          -2.5279e-02, -3.2336e-01, -4.7012e-02, -2.9052e-01, -3.4413e-01,\n",
      "          -8.0064e-01, -3.8177e-02],\n",
      "         [ 3.4540e-01,  5.6372e-02, -8.2485e-01, -2.4862e-01, -2.7683e+00,\n",
      "           1.2607e+01,  3.7248e-02,  1.9025e+00, -1.7364e-01, -1.5517e+00,\n",
      "          -5.4494e+00,  1.4025e+00,  2.1003e+00,  7.5182e-01, -6.8520e-01,\n",
      "          -3.7602e-03, -5.0764e-01, -8.0647e-01, -5.6342e-01, -1.3423e-01,\n",
      "          -1.5149e-01, -6.7609e-02, -1.8244e-01, -5.3189e-02, -1.0693e-01,\n",
      "          -1.9486e-01,  4.5976e-02,  1.8167e-01, -3.6973e-01, -9.7039e-02,\n",
      "          -1.0949e-01, -7.0025e-01, -1.5135e-01, -1.6018e-01, -6.3086e-02,\n",
      "          -3.7033e-01, -1.1984e-01, -4.2549e-01, -3.5671e-01, -7.2805e-01,\n",
      "          -1.2475e-02, -4.4681e-02,  2.5317e-01, -5.4805e-01, -1.7697e-01,\n",
      "          -3.6495e-02, -1.0597e-01, -3.6834e-01, -3.2112e-01,  1.7639e-01,\n",
      "          -6.6198e-01, -1.9117e-01],\n",
      "         [-5.8010e-01, -7.5991e-01, -7.8305e-01, -6.3534e-01,  6.1045e-02,\n",
      "           7.1830e-01, -8.8035e-01,  3.2208e+00,  1.0405e+01,  2.3710e+00,\n",
      "           1.9907e+00, -2.2750e+00,  2.4629e+00, -3.3091e+00,  1.0725e+01,\n",
      "          -7.0281e-01, -5.9378e-01, -7.2261e-01, -3.4190e-01, -6.7389e-01,\n",
      "          -5.0382e-01, -6.1336e-01, -6.6025e-01, -7.5016e-01, -4.4937e-01,\n",
      "          -6.2616e-01, -7.4266e-01, -5.8193e-01, -3.7795e-01, -5.4118e-01,\n",
      "          -2.6252e-01, -7.4234e-01, -1.0539e+00, -6.2685e-01, -9.0412e-01,\n",
      "          -6.8878e-01, -6.1091e-01, -4.9648e-01, -6.7708e-01, -5.9737e-01,\n",
      "          -6.2473e-01, -5.5369e-01, -5.4127e-01, -8.3813e-01, -9.3853e-01,\n",
      "          -6.2830e-01, -4.7997e-01, -2.8550e-01, -9.7540e-01, -6.0877e-01,\n",
      "          -6.9148e-01, -3.5506e-01],\n",
      "         [-7.0128e-01, -5.2434e-01, -6.1177e-01, -8.0395e-01,  2.8607e+00,\n",
      "           2.0564e+00, -6.5989e-01,  9.9241e+00,  1.9111e+00,  3.0852e+00,\n",
      "           1.9639e+00, -4.7532e-01, -1.0665e-02,  1.0301e+01, -3.4456e+00,\n",
      "          -8.0416e-01, -9.2456e-01, -4.5855e-01, -9.7977e-01, -5.8741e-01,\n",
      "          -6.0352e-01, -6.4303e-01, -4.4637e-01, -8.5339e-01, -7.5986e-01,\n",
      "          -8.8974e-01, -6.8272e-01, -7.1666e-01, -1.0547e+00, -7.0202e-01,\n",
      "          -7.0570e-01, -8.5301e-01, -5.1639e-01, -3.1417e-01, -6.2079e-01,\n",
      "          -3.4237e-01, -8.6597e-01, -5.6537e-01, -6.1064e-01, -7.7548e-01,\n",
      "          -4.7873e-01, -5.7648e-01, -6.9146e-01, -4.2596e-01, -5.3433e-01,\n",
      "          -7.6623e-01, -8.2723e-01, -1.0433e+00, -3.8323e-01, -7.2131e-01,\n",
      "          -1.0209e+00, -6.1875e-01],\n",
      "         [-6.6440e-01, -3.8746e-01, -5.3213e-01, -6.7169e-01,  1.0354e+01,\n",
      "          -1.5968e+00, -8.5843e-01,  1.0013e+01,  2.6229e+00,  2.4345e+00,\n",
      "           1.6973e+00, -1.1153e+00,  8.9818e-01,  2.8363e+00, -4.7228e-01,\n",
      "          -7.6221e-01, -4.2341e-01, -5.2087e-01, -6.3209e-01, -4.6236e-01,\n",
      "          -5.8025e-01, -3.0685e-01, -7.8099e-01, -3.1812e-01, -8.7550e-01,\n",
      "          -6.8204e-01, -4.4644e-01, -7.5352e-01, -8.0030e-01, -4.3888e-01,\n",
      "          -6.6688e-01, -3.8042e-01, -3.2248e-01, -9.2012e-01, -5.8181e-01,\n",
      "          -5.0800e-01, -6.5013e-01, -6.3404e-01, -5.4475e-01, -5.8492e-01,\n",
      "          -1.0046e+00, -5.9578e-01, -6.0418e-01, -8.0672e-01, -7.6837e-01,\n",
      "          -7.3788e-01, -4.0629e-01, -4.6666e-01, -5.0661e-01, -7.8052e-01,\n",
      "          -3.5474e-01, -8.7317e-01],\n",
      "         [-9.6297e-01, -9.3602e-01, -7.6290e-01, -7.2349e-01, -2.0353e+00,\n",
      "           1.0515e+00, -4.9027e-01,  4.4867e+00,  3.8657e+00,  1.1224e+01,\n",
      "           3.5668e+00,  1.1763e+01,  1.6408e+00, -6.1633e-01, -9.9112e-01,\n",
      "          -5.5415e-01, -7.9784e-01, -9.0528e-01, -7.7497e-01, -8.0196e-01,\n",
      "          -9.8720e-01, -1.0700e+00, -7.8248e-01, -8.7676e-01, -7.3629e-01,\n",
      "          -7.6334e-01, -8.3274e-01, -8.2520e-01, -5.4213e-01, -1.2211e+00,\n",
      "          -9.7423e-01, -6.8834e-01, -7.8141e-01, -8.5002e-01, -7.9255e-01,\n",
      "          -1.0340e+00, -6.4829e-01, -8.4869e-01, -1.0019e+00, -8.4700e-01,\n",
      "          -8.0604e-01, -9.2880e-01, -9.4466e-01, -6.9991e-01, -5.9585e-01,\n",
      "          -5.9806e-01, -1.0127e+00, -1.0409e+00, -9.4517e-01, -8.7740e-01,\n",
      "          -7.8078e-01, -8.5869e-01],\n",
      "         [-5.8010e-01, -7.5991e-01, -7.8305e-01, -6.3534e-01,  6.1045e-02,\n",
      "           7.1830e-01, -8.8035e-01,  3.2208e+00,  1.0405e+01,  2.3710e+00,\n",
      "           1.9907e+00, -2.2750e+00,  2.4629e+00, -3.3091e+00,  1.0725e+01,\n",
      "          -7.0281e-01, -5.9378e-01, -7.2261e-01, -3.4190e-01, -6.7389e-01,\n",
      "          -5.0382e-01, -6.1336e-01, -6.6025e-01, -7.5016e-01, -4.4937e-01,\n",
      "          -6.2616e-01, -7.4266e-01, -5.8193e-01, -3.7795e-01, -5.4118e-01,\n",
      "          -2.6252e-01, -7.4234e-01, -1.0539e+00, -6.2685e-01, -9.0412e-01,\n",
      "          -6.8878e-01, -6.1091e-01, -4.9648e-01, -6.7708e-01, -5.9737e-01,\n",
      "          -6.2473e-01, -5.5369e-01, -5.4127e-01, -8.3813e-01, -9.3853e-01,\n",
      "          -6.2830e-01, -4.7997e-01, -2.8550e-01, -9.7540e-01, -6.0877e-01,\n",
      "          -6.9148e-01, -3.5506e-01],\n",
      "         [-7.0128e-01, -5.2434e-01, -6.1177e-01, -8.0395e-01,  2.8607e+00,\n",
      "           2.0564e+00, -6.5989e-01,  9.9241e+00,  1.9111e+00,  3.0852e+00,\n",
      "           1.9639e+00, -4.7532e-01, -1.0666e-02,  1.0301e+01, -3.4456e+00,\n",
      "          -8.0416e-01, -9.2456e-01, -4.5855e-01, -9.7977e-01, -5.8741e-01,\n",
      "          -6.0352e-01, -6.4303e-01, -4.4637e-01, -8.5339e-01, -7.5986e-01,\n",
      "          -8.8974e-01, -6.8272e-01, -7.1666e-01, -1.0547e+00, -7.0202e-01,\n",
      "          -7.0570e-01, -8.5301e-01, -5.1639e-01, -3.1417e-01, -6.2079e-01,\n",
      "          -3.4237e-01, -8.6597e-01, -5.6537e-01, -6.1064e-01, -7.7548e-01,\n",
      "          -4.7873e-01, -5.7648e-01, -6.9146e-01, -4.2596e-01, -5.3433e-01,\n",
      "          -7.6623e-01, -8.2723e-01, -1.0433e+00, -3.8323e-01, -7.2131e-01,\n",
      "          -1.0209e+00, -6.1875e-01],\n",
      "         [-6.6440e-01, -3.8746e-01, -5.3213e-01, -6.7169e-01,  1.0354e+01,\n",
      "          -1.5968e+00, -8.5843e-01,  1.0013e+01,  2.6229e+00,  2.4345e+00,\n",
      "           1.6973e+00, -1.1153e+00,  8.9818e-01,  2.8363e+00, -4.7228e-01,\n",
      "          -7.6221e-01, -4.2341e-01, -5.2087e-01, -6.3209e-01, -4.6236e-01,\n",
      "          -5.8025e-01, -3.0685e-01, -7.8099e-01, -3.1812e-01, -8.7550e-01,\n",
      "          -6.8204e-01, -4.4644e-01, -7.5352e-01, -8.0030e-01, -4.3888e-01,\n",
      "          -6.6688e-01, -3.8042e-01, -3.2248e-01, -9.2012e-01, -5.8181e-01,\n",
      "          -5.0800e-01, -6.5013e-01, -6.3404e-01, -5.4475e-01, -5.8492e-01,\n",
      "          -1.0046e+00, -5.9578e-01, -6.0418e-01, -8.0672e-01, -7.6837e-01,\n",
      "          -7.3788e-01, -4.0629e-01, -4.6666e-01, -5.0661e-01, -7.8052e-01,\n",
      "          -3.5474e-01, -8.7317e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 15\n",
    "file_tokens = tokens_from_file(input_text)\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    input_x = file_tokens[:, 0:INPUT_SIZE]\n",
    "    target_y = file_tokens[0, INPUT_SIZE:INPUT_SIZE + INPUT_SIZE] # todo: batch x/y\n",
    "    output = forward_pass(input_x)\n",
    "    print(output)\n",
    "    break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output.view(-1, len(char_to_token)), target_y)\n",
    "    print('loss={}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=EMBEDDING_SIZE, nhead=10)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(1, 750, EMBEDDING_SIZE)\n",
    "transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder.layers[0].self_attn.out_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_token['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = 'abcdefghiklmnopqrstuvwxyz '\n",
    "def tokenize(s):\n",
    "    return [dictionary.index(c) for c in s]\n",
    "embed = nn.Embedding(len(dictionary), 3)\n",
    "embed(torch.LongTensor(tokenize('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed(torch.LongTensor([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.index('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "max_len = 8\n",
    "d_model = 4\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
